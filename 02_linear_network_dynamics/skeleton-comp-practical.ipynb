{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models of Higher Brain Functions - Computer Practical\n",
    "## 'Learning Dynamics in Deep Linear Networks' - November 2019 - TU Berlin\n",
    "\n",
    "# Skeleton for Exercise Two:  Deeper (Non-)Linear Networks with AutoDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "!pip install torch==1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "import numpy as np\n",
    "from numpy.linalg import svd as svd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Branching Diffusion Process for Data Generation\n",
    "\n",
    "The `DiffuseTreeSampler`Class implements a hierarchical data-generation process which we use throughout this exercise (**no need to implement anything from scratch**). A target is generated for all datapoints at a time in a tree-based sequential fashion. A first +/-1 coin flip determines the initial sign of the node at the top of the tree. Afterwards, the tree branches and changes the sign at each stage with a small probability. We repeat this branching process for each layer of the tree. The bottom of the tree corresponds to the value of one feature across the different datapoints. In order to generate multiple target dimensions, the process is repeated for each dimension independently. Finally, we stack them into the overall targets. The input again corresponds to the identity matrix. For more information you can have a look here: https://arxiv.org/pdf/1810.10531.pdf (p. 14). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffuseTreeSampler():\n",
    "    def __init__(self, target_dim, tree_depth, branching_factor, sample_epsilon):\n",
    "        self.target_dim = target_dim\n",
    "        self.num_examples = branching_factor**tree_depth\n",
    "        self.tree_depth = tree_depth\n",
    "        self.branching_factor = branching_factor\n",
    "        self.sample_epsilon = sample_epsilon\n",
    "    \n",
    "    def sample_target(self):\n",
    "        samples_per_tree_layer = [self.branching_factor**i for i in range(1, self.tree_depth+1)] \n",
    "        target_tree = [np.random.choice([-1, 1], p=[0.5, 0.5], size=1)]\n",
    "        for l in range(self.tree_depth):\n",
    "            switch = np.random.choice([-1, 1],\n",
    "                                      p=[self.sample_epsilon, 1-self.sample_epsilon],\n",
    "                                      size=samples_per_tree_layer[l])\n",
    "            next_layer = np.repeat(target_tree[-1], self.branching_factor)\n",
    "            target_tree.append(next_layer*switch)\n",
    "        return target_tree[-1]\n",
    "    \n",
    "    def sample_data(self):\n",
    "        \"\"\"\n",
    "        Each target dimension diffuses independently of the others!\n",
    "        \"\"\"\n",
    "        targets = []\n",
    "        for tar in range(self.target_dim):\n",
    "            target_temp = self.sample_target()\n",
    "            targets.append(target_temp)\n",
    "        \n",
    "        targets_out = np.array(targets).T\n",
    "        features_out = np.diag(np.ones(self.num_examples))\n",
    "        return targets_out, features_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Dimensions: (64, 100)\n",
      "Input Dimensions: (64, 64)\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset by instantiating and sampling\n",
    "hierarchical_tree = DiffuseTreeSampler(target_dim=100, tree_depth=3,\n",
    "                                       branching_factor=4, sample_epsilon=0.5)\n",
    "targets, features = hierarchical_tree.sample_data()\n",
    "\n",
    "print(\"Output Dimensions: {}\".format(targets.shape))\n",
    "print(\"Input Dimensions: {}\".format(features.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the SVD of the covariance matrix\n",
    "SIGMA_YX = targets.T @ features\n",
    "U, s, V = svd(SIGMA_YX, full_matrices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. PyTorch Example - Feedforward Networks\n",
    "* Define Neural Network Architecture (single hidden layer ReLU activation)\n",
    "* Create a Network Instance, Optimizer (stochastic gradient descent) & MSE (mean squared error) Loss\n",
    "* Perform a forward pass to get predictions, calculate the loss\n",
    "* Reset the gradients to 0 and perform a backward pass to calculate the gradients + (SGD) update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Feedforward PyTorch Network\n",
    "class DeepNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Network in PyTorch - Single Hidden Layer with ReLU activation\n",
    "    Inputs: Input Array Dimensions, Output Array Dimensions\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DeepNet, self).__init__()\n",
    "        # Define a dictionary that collects the different layers \n",
    "        # Afterwards, this dictionary provides the input to the nn.Sequential model\n",
    "        layers = OrderedDict()\n",
    "        layers[\"in_hidden\"] = nn.Linear(input_dim, 64, bias=False)\n",
    "        layers[\"in_hidden-activation\"] = nn.ReLU()\n",
    "        layers[\"hidden_out\"] = nn.Linear(64, output_dim, bias=False)\n",
    "        self.model = nn.Sequential(layers)\n",
    "        \n",
    "    def forward(self, input_array):\n",
    "        # Propagate the input through the linear network\n",
    "        return self.model(input_array.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Network Architecture\n",
      "DeepNet(\n",
      "  (model): Sequential(\n",
      "    (in_hidden): Linear(in_features=64, out_features=64, bias=False)\n",
      "    (in_hidden-activation): ReLU()\n",
      "    (hidden_out): Linear(in_features=64, out_features=100, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the network instance, define the learning rate, optimizer & loss\n",
    "input_dim, output_dim = features.shape[1], targets.shape[1]\n",
    "\n",
    "l_rate = 0.5\n",
    "relu_net = DeepNet(input_dim, output_dim)\n",
    "relu_optimizer = optim.SGD(relu_net.parameters(), lr=l_rate)\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "print(\"The Network Architecture\")\n",
    "print(relu_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE Loss for the 1st datapoint:0.989\n"
     ]
    }
   ],
   "source": [
    "# Perform a forward pass through the network and calculate the loss\n",
    "input_tensor = torch.tensor(features[0])\n",
    "y_true = torch.tensor(targets[0]).float()\n",
    "y_hat = relu_net(input_tensor)\n",
    "loss = mse_loss(y_hat, y_true)\n",
    "\n",
    "print(\"The MSE Loss for the 1st datapoint:{:.3f}\".format(loss))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Sum Input-Hidden Weights after reset: None\n",
      "Gradient Sum Input-Hidden Weights after backward pass: -0.09603124856948853\n"
     ]
    }
   ],
   "source": [
    "# Perform a backward pass and update the weights using the SGD optimizer\n",
    "relu_net.zero_grad()\n",
    "print(\"Gradient Sum Input-Hidden Weights after reset: {}\".format(relu_net.model.in_hidden.weight.grad))\n",
    "loss.backward()\n",
    "print(\"Gradient Sum Input-Hidden Weights after backward pass: {}\".format(relu_net.model.in_hidden.weight.grad.sum()))\n",
    "relu_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Code a Variable Depth Linear Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generalize to variable depth\n",
    "class DeepLinearNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Network in PyTorch - Single Hidden Layer with ReLU activation\n",
    "    Inputs: Input Array Dimensions, Output Array Dimensions\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=64, output_dim=100, hidden_units=[64]):\n",
    "        super(DeepNet, self).__init__()\n",
    "        # Define a dictionary that collects the different layers \n",
    "        # Afterwards, this dictionary provides the input to the nn.Sequential model\n",
    "        layers = OrderedDict()\n",
    "        layers[\"in-hidden\"] = nn.Linear(input_dim, hidden_units[0], bias=False)\n",
    "        # TODO: Loop over the hidden layers in the middle to define the full network architecture!\n",
    "        layers[\"hidden-out\"] = nn.Linear(hidden_units[-1], output_dim, bias=False)\n",
    "        self.model = nn.Sequential(layers)\n",
    "        \n",
    "    def forward(self, input_array):\n",
    "        # Propagate the input through the linear network\n",
    "        return self.model(input_array.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Code the Learning Loop for the Linear Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the Online Gradient Descent Training Loop\n",
    "def linear_net_learning(deep_net, mse_loss, optimizer, num_epochs, features, targets):\n",
    "    \"\"\"\n",
    "    Inputs: 'deep_net'   - Instantiated PyTorch Network\n",
    "            'mse_loss'   - Loss Criterion, i.e. Mean Squared Error\n",
    "            'optimizer'  - PyTorch Optimizer Object, i.e. Stochastic Gradient Descent\n",
    "            'num_epochs' - Number of Training Loops over the entire dataset\n",
    "            'features'   - Training features generated from the sampler\n",
    "            'targets'    - Training targets generated from the sampler\n",
    "    Function: Runs the learning loop for the linear network\n",
    "    \"\"\"\n",
    "    loss_log = []\n",
    "    log_singular_vals = []\n",
    "    num_points = targets.shape[0]\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        # TODO: Reshuffle the order of the dataset\n",
    "        \n",
    "        # Loop over all examples in an Online SGD Loop\n",
    "        for t in range(num_points):\n",
    "            # Extract the current training datapoint and transform it into a Torch Tensor\n",
    "            input_tensor = torch.tensor(train_items[t])\n",
    "            y_true = torch.tensor(train_features[t]).float()\n",
    "                \n",
    "            # TODO: Compute the prediction for the single datapoint y^hat & the corresponding loss\n",
    "                     \n",
    "            # TODO: Clear the gradients, Perform the backward pass, and SGD update\n",
    "            \n",
    "            # Update the epoch loss tracker\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Log the mean epoch loss & calculate the SVD\n",
    "        loss_log.append(epoch_loss/num_points)\n",
    "        y_hat_full = deep_net(torch.tensor(features)).detach().numpy()\n",
    "        U, s, V = svd(y_hat_full.T, full_matrices=True)\n",
    "        log_singular_vals.append(s)\n",
    "    return loss_log, np.array(log_singular_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run the Loop & Plot the singular values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate the network, define the loss & optimizer and run the learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the singular values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Add ReLU Non-Linearities to the Architecture & Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define a new network class that adds a ReLU activation function after each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate the network, define the loss & optimizer and run the learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the singular values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (saxe)",
   "language": "python",
   "name": "saxe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
